{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base path the active DataARC API\n",
    "BASE_URL = \"https://api.data-arc.org\"\n",
    "\n",
    "## A basic function to generate a path based on the\n",
    "## documentation located at https://api.data-arc.org/documentation\n",
    "## example: MAKE_PATH(\"datasets\")\n",
    "MAKE_PATH = lambda path: f\"{BASE_URL}/{path}\"\n",
    "\n",
    "# End point URIs\n",
    "DATASET_END_POINT = \"datasets\"\n",
    "DATASET_FEATURES_END_POINT = \"features\"\n",
    "\n",
    "# Utility Functions\n",
    "\n",
    "def get_dataset_by_name(name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Simple method for quickly querying and filtering a dataset\n",
    "    based on a name\n",
    "\n",
    "    params\n",
    "    ------\n",
    "    name: str\n",
    "      The string name as it is in the database\n",
    "    \n",
    "    returns\n",
    "    -------\n",
    "    dict\n",
    "      The returned JSON data from the API\n",
    "    \"\"\"\n",
    "    datasets = requests.get(MAKE_PATH(DATASET_END_POINT)).json()\n",
    "    dataset = list(filter(lambda d: d.get('name') == name, datasets))[0]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def map_feature_to_nested_property(feature, prop='properties'):\n",
    "    \"\"\"\n",
    "    This is a basic function to extract nested dictionaries from features\n",
    "    returned from the DataARC database. This is not the same as flattening\n",
    "    as only the specified nested dictionary is extracted. \n",
    "    The original feature id as kept as `_id` in case future REST queries\n",
    "    require this database specific id to be in place\n",
    "\n",
    "    params\n",
    "    ------\n",
    "    feature: dict\n",
    "      the feature returned from the DataARC API\n",
    "    \n",
    "    prop: str\n",
    "      the nested property to be returned - usually a dictionary value\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    dict\n",
    "      the extracted nested property with the _id in place\n",
    "    \"\"\"\n",
    "    properties = feature.get(prop, {})\n",
    "    ## Capture the database ID to run additional feature queries\n",
    "    properties['_id'] = feature.get('id') \n",
    "    return properties\n",
    "\n",
    "\n",
    "def get_combinators_from_features(features: list) -> dict:\n",
    "    \"\"\"\n",
    "    Collect combinators based on a unique list of feature _id values\n",
    "\n",
    "    params\n",
    "    ------\n",
    "    features: List[str]\n",
    "      a unique list of str _id values resulted from the /features api\n",
    "    \n",
    "    returns\n",
    "    -------\n",
    "    dict\n",
    "      a dictionary containing both the found related and contextual unique\n",
    "      ids as properties\n",
    "    \"\"\"\n",
    "    # Use the original feature _id to collect concepts through their combinators\n",
    "    data = requests.get(MAKE_PATH(\"combinators\"), {\"features_in\": features}).json()\n",
    "    # >> List[List[Dict]]\n",
    "    # Combinators using the `_in` suffix will return a list of lists with dictionaries\n",
    "    combinator_collection = [combinator.get('concepts') for combinator in data]\n",
    "\n",
    "    related = []\n",
    "    contextual = []\n",
    "\n",
    "    for combinators in combinator_collection:\n",
    "        for combinator in combinators:\n",
    "            related += combinator.get('related')\n",
    "            contextual += combinator.get('contextual')\n",
    "    \n",
    "    return {\n",
    "        'related': set(related),\n",
    "        'contextual': set(contextual)\n",
    "    }\n",
    "\n",
    "\n",
    "def make_concepts_request(concept_ids: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collect concepts from their _id values\n",
    "\n",
    "    params\n",
    "    ------\n",
    "    concept_ids: List[str]\n",
    "      The id returned from the API end point for /concepts\n",
    "    \n",
    "    returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "      A precompiled DataFrame with the concept results\n",
    "    \"\"\"\n",
    "    data = requests.get(MAKE_PATH(\"concepts\"), {\n",
    "        \"id_in\": concept_ids\n",
    "    }).json()\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def get_features_by_dataset_id(dataset_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a dataset id (_id), construct a DataFrame with the results\n",
    "\n",
    "    params\n",
    "    ------\n",
    "    dataset_id: str\n",
    "      the dataset id (or _id) returned from the /datasets endpoint\n",
    "    \n",
    "    returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "      the constructed DataFrame\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"_limit\": -1 ## This will retrieve all records without a limit\n",
    "    }\n",
    "\n",
    "    features_data = requests.get(MAKE_PATH(DATASET_FEATURES_END_POINT), params=params).json()\n",
    "\n",
    "    return pd.DataFrame(map(map_feature_to_nested_property, features_data))\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Use case 1: \n",
    "I'm interested in different communities' reliance on marine vs. terrestrial resources for food. I want to find places where there's a fairly balanced reliance on these. As a ballpark figure, I'd start by looking at places where the ration is something between 40/60 and 60/40 marine to terrestrial."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Step 1: I know from exploring on the front end UI that nabonosead has some of the data I want... I've seen indicators of both marine and terrestrial resources in that data when I searched on a concept about 'livestock'. I head to the 'advanced user interface' aka this jupyter notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Step 2: I make an api call to get the list of datasets (so I know what the nabonosead dataset is called in the API) and another API call to get the list of field names in the nabonosead data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the nabonosead dataset dictionary\n",
    "nabonosead_dataset = get_dataset_by_name('nabonosead')\n",
    "# The dataset has all fields in the initial return - this can be extracted from the dict\n",
    "nabonosead_dataset_fields = nabonosead_dataset.get('fields')\n",
    "# Create a DataFrame from the extracted fields\n",
    "nabonosead_fields_df = pd.DataFrame(nabonosead_dataset_fields)\n",
    "nabonosead_fields_df"
   ]
  },
  {
   "source": [
    "Step 3: I can see that Indicators Freshwater Fish, Indicators Marine Fish, Indicators Domestic and Indicators Wild seem like likely candidate fields to point at sites I'd want to investigate."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Step 4: I want to construct my query to get sites with a balanced ratio... So I start by trying an API query that gets all the sites where Indicators Freshwater Fish OR Indicators Marine Fish OR Indicators Domestic OR Indicators Wild >0 and put them into a dataframe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the dataset id from the above dictionary to collect the features\n",
    "# For this we can call our handy function from above\n",
    "features_df = get_features_by_dataset_id(nabonosead_dataset.get('id'))\n",
    "\n",
    "# Sub-select DataFrame with our conditional OR '|' operations\n",
    "sub_features = features_df[\n",
    "    (features_df['indicators_wild'] > 0) | \n",
    "    (features_df['indicators_marine_fish'] > 0) | \n",
    "    (features_df['indicators_domestic'] > 0) | \n",
    "    (features_df['indicators_freshwater_fish'] > 0)\n",
    "]"
   ]
  },
  {
   "source": [
    "Step 5: That's given me more sites than what I really want (balanced and unbalanced ratios are included). But I can filter the dataframe using the magic of pandas. I do \n",
    "\n",
    "```\n",
    "df.mydataset = df.loc[\n",
    "    (Indicators Freshwater Fish + Indicators Marine Fish) / (Indicators Domestic + Indicators Wild) > 40 & \n",
    "    (Indicators Freshwater Fish + Indicators Marine Fish) / (Indicators Domestic + Indicators Wild) < 60\n",
    "]\n",
    "``` \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in a new DataFrame with the `ratio` applied\n",
    "features_with_ratio_df = sub_features.assign(\n",
    "    ratio=(sub_features['indicators_freshwater_fish'] + sub_features['indicators_marine_fish']) / \\\n",
    "          (sub_features['indicators_domestic'] + sub_features['indicators_wild'])\n",
    ")\n",
    "# Drop any NaN values resulting from division by 0\n",
    "features_with_a_valid_ratio = features_with_ratio_df.dropna(subset=[\"ratio\"])\n",
    "\n",
    "# Collect the balanced features remaining\n",
    "balanced = features_with_a_valid_ratio[\n",
    "    (features_with_a_valid_ratio['ratio'] > 0.4) & (features_with_a_valid_ratio['ratio'] < 0.6)\n",
    "]\n",
    "balanced"
   ]
  },
  {
   "source": [
    "Step 6: I am happy and maybe make myself some nice charts showing the different ratios in my subset of sites with balanced ratios..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{len(balanced)} features found\""
   ]
  },
  {
   "source": [
    "Step 7: I wonder what other data might be connected via concepts and also relevant to these sites with balanced resource use. I run an API call against the ids of these items from nabonosead to get the set of concepts that are 'related' and 'contextual' to their concepts <-- this part I'm not sure how you have it set up in the API."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the unique list of balanced feature _id values for the API\n",
    "unique_feature_ids = set(balanced['_id'])\n",
    "\n",
    "# To get concepts, we must first travel through the combinators\n",
    "# Collect combinators\n",
    "combinators = get_combinators_from_features(unique_feature_ids)\n",
    "\n",
    "# For this we want the `related` and `contextual` combinators\n",
    "related = combinators.get('related')\n",
    "contextual = combinators.get('contextual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the combinator ids to collect concepts using another handy function\n",
    "related_df = make_concepts_request(related)\n",
    "contextual_df = make_concepts_request(contextual)"
   ]
  },
  {
   "source": [
    "Step 8: I could go back to the UI and search on these concepts, or I could search for the datasets linked to one of the related concepts from here..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Use case: \n",
    "I'm interested in looking at places with lots of data from short well defined timespans. From looking around in the UI, I can see the tephrabase data should let me find places with well defined chronologies and then I could see what other data is available in these places and how much of it there is."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Step 1: I do an API query to get the list of dataset names and another to get the list of field names in the tephrabase dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the tephrabase dictioary\n",
    "tephrabase_dataset = get_dataset_by_name('tephrabase')\n",
    "# Extract the fields from the dictionary directly\n",
    "tephrabase_dataset_fields = tephrabase_dataset.get('fields', [])\n",
    "# Construct the DataFrame to inspect the fields\n",
    "tephrabase_dataset_fields_df = pd.DataFrame(tephrabase_dataset_fields)"
   ]
  },
  {
   "source": [
    "Step 2: I can see the earliest date and latest date fields in the tephrabase dataset. I construct an API query to get the tephrabase data into a dataframe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the `id` within the dataset dictionary, collect all the features\n",
    "# Again, we'll use our handy function for this\n",
    "tephrabase_features_df = get_features_by_dataset_id(tephrabase_dataset.get('id'))"
   ]
  },
  {
   "source": [
    "Step 3: I filter my dataframe to select rows where df.mydataset = df.loc[tephrabase(latest date - earliest date)<100] to get sequences of less than 100 years"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the conditions that we only need less than 100 year span\n",
    "# From the fields above we see it's `latestyear` and `earliestyear`\n",
    "tephrabase_less_than_100 = tephrabase_features_df[\n",
    "    (tephrabase_features_df['latestyear'] - tephrabase_features_df['earliestyear']) < 100\n",
    "]\n",
    "tephrabase_less_than_100"
   ]
  },
  {
   "source": [
    "Step 4: Assuming this gives me a reasonable set of sites, I do another API query to see what concepts are mapped to these sites."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the unique list of balanced feature _id values for the API\n",
    "unique_feature_ids = set(tephrabase_less_than_100['_id'])\n",
    "\n",
    "# To get concepts, we must first travel through the combinators\n",
    "# Collect combinators\n",
    "combinators = get_combinators_from_features(unique_feature_ids)\n",
    "\n",
    "# For this we want the `related` and `contextual` combinators\n",
    "related = combinators.get('related')\n",
    "contextual = combinators.get('contextual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the combinator ids to collect concepts using another handy function\n",
    "related_df = make_concepts_request(related)\n",
    "contextual_df = make_concepts_request(contextual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_df"
   ]
  },
  {
   "source": [
    "Step 5: I go back to the UI and apply a spatial filter to look at the specific sites I've identified here and see what other data is related to them AND is mapped to the same concepts (by filtering by concept)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}